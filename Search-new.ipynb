{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert json\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json data\n",
    "url = \"http://api.apomden.com/v2/facilities\"\n",
    "hospitals=requests.get(url).json()\n",
    "hospitals = hospitals[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_FIELDS = [\"name\", \"street\", \"city\", \"district\", \"regions\", \"services\"]\n",
    "\n",
    "def getTokens(sentence, tokenizer):\n",
    "    return tokenizer.tokenize(sentence)\n",
    "\n",
    "def extractJson(list_dicts, field_list):\n",
    "    \n",
    "    json_tokens = {field:[] for field in field_list}\n",
    "    \n",
    "    for item in list_dicts:\n",
    "        for field in SEARCH_FIELDS:\n",
    "            x = extract(item, field)\n",
    "            json_tokens[field].extend(x)\n",
    "    return json_tokens\n",
    "\n",
    "def extract(obj, key):\n",
    "    \n",
    "    tokens = []\n",
    "    if isinstance(obj,(str,bool)):\n",
    "        return [obj]\n",
    "    elif isinstance(obj,dict):\n",
    "        for k,v in obj.items():\n",
    "            if k==key:\n",
    "                tokens.append(v)\n",
    "            elif isinstance(v, dict):\n",
    "                if k in v.keys():\n",
    "                    tokens.extend(extract(v, key))\n",
    "            elif isinstance(v,(dict,list)):\n",
    "                tokens.extend(extract(v,key))\n",
    "                \n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            tokens.extend(extract(item,key))\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def vectorizeTokens(token_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a numpy matrix of vectorized tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def buildIndices(token_list, key=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns dictionaries of variable to idx and vice versa pairing.\n",
    "    token_list: list\n",
    "    key = string\n",
    "    \"\"\"\n",
    "              \n",
    "    var_to_index = {}\n",
    "    index_to_var = {}\n",
    "    \n",
    "    if key is not None:\n",
    "        for idx, item in enumerate(token_list):\n",
    "            var_to_index[item[key]] = idx\n",
    "            index_to_var[idx] =  item[key]      \n",
    "\n",
    "    else:\n",
    "        for idx, item in enumerate(token_list):\n",
    "            var_to_index[item] = idx\n",
    "            index_to_var[idx] = item\n",
    "\n",
    "    return var_to_index, index_to_var\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(d, exclude):\n",
    "    if not(isinstance(d, list)) and not(isinstance(d, dict)):\n",
    "        return [d]\n",
    "    tokens = []\n",
    "    if isinstance(d, list):\n",
    "        for i in d:\n",
    "            tokens.extend(get_all_tokens(i,exclude))\n",
    "    elif isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if k not in exclude:\n",
    "                tokens.extend(get_all_tokens(v,exclude))\n",
    "    return tokens\n",
    "\n",
    "def extract2(d, exclude):\n",
    "    tokens = []\n",
    "    if isinstance(d, list):\n",
    "        for i in d:\n",
    "            tokens.extend(get_all_tokens(i,exclude))\n",
    "    elif isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if k not in exclude:\n",
    "                tokens.extend(extract2(v, k))\n",
    "    return tokens\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    for k,v in d.items():\n",
    "        print(f\"------ {k} -----\")\n",
    "        if isinstance(v, list):\n",
    "            for i in v:\n",
    "                print(i)\n",
    "        else:\n",
    "            print(v)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocuments(list_dicts, exclude):\n",
    "    \n",
    "    def extractDocs(obj,key):\n",
    "        if key not in exclude:\n",
    "            if isinstance(obj,str):\n",
    "                return [obj]\n",
    "            elif isinstance(obj,(dict,list)) :\n",
    "                x = extract2(obj,exclude)\n",
    "                return x\n",
    "\n",
    "    docs = {}\n",
    "    all_words = []\n",
    "    for idx,doc in enumerate(list_dicts):\n",
    "        d = []\n",
    "        for k,v in doc.items():\n",
    "            if k not in exclude:\n",
    "                #print(k)\n",
    "                if not isinstance(v,(dict,list)):\n",
    "                    d.append(v)\n",
    "                    all_words.append(v)\n",
    "                else:\n",
    "                    val = extractDocs(v,k)\n",
    "                    d.extend(val)\n",
    "                    all_words.extend(val)\n",
    "        docs[idx] = d     \n",
    "\n",
    "    return docs, all_words\n",
    "\n",
    "\n",
    "def convert_to_lowercase(docs):  \n",
    "    new_docs = {}\n",
    "    for ix, lst in docs.items():\n",
    "        new_lst = []\n",
    "        for x in lst:\n",
    "            new_lst.extend(x.split())\n",
    "        new_docs[ix] = list(map(lambda x:x.lower(),new_lst))\n",
    "    return new_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTokens(wordlist, stopwords):\n",
    "    x = []\n",
    "    for word in wordlist:\n",
    "        if isinstance(word,str) and word not in stopwords:\n",
    "            x.extend([wd.lower() for wd in word.strip().split()])\n",
    "    return x\n",
    "\n",
    "def inverted_index(docs):\n",
    "    \n",
    "    inverted_index=defaultdict(list)\n",
    "    \n",
    "    for ix, doc in docs.items():\n",
    "        for word in doc:\n",
    "            if ix not in inverted_index[word]:\n",
    "                inverted_index[word].append(ix)\n",
    "    \n",
    "    return inverted_index  \n",
    "\n",
    "def retrieve_docs(query, inverted_index, tokenizer):\n",
    "    \n",
    "    query_words = [word.lower() for word in getTokens(query, tokenizer)]\n",
    "    q_len = len(query_words)\n",
    "    \n",
    "    intersection = None\n",
    "    union = None\n",
    "    \n",
    "    if q_len%2 == 0:\n",
    "        for i in range(0,q_len,2):\n",
    "            postings1 = inverted_index[query_words[i]]\n",
    "            postings2 = inverted_index[query_words[i+1]]\n",
    "            \n",
    "            new_intersection = np.intersect1d(postings1, postings2)\n",
    "            \n",
    "            if intersection is None:\n",
    "                intersection = new_intersection\n",
    "            else:\n",
    "                intersection = np.intersect1d(intersection, new_intersection)\n",
    "            \n",
    "            new_union = np.union1d(postings1, postings2)\n",
    "            \n",
    "            if union is None:\n",
    "                union = new_union \n",
    "            else: \n",
    "                union = np.union1d(union, new_union)\n",
    "    else:\n",
    "        for i in range(0, q_len-1,2):\n",
    "            postings1 = inverted_index[query_words[i]]\n",
    "            postings2 = inverted_index[query_words[i+1]]\n",
    "            \n",
    "            new_intersection = np.intersect1d(postings1, postings2)\n",
    "            \n",
    "            if intersection is None:\n",
    "                intersection = new_intersection\n",
    "            else:\n",
    "                intersection = np.intersect1d(intersection, new_intersection)\n",
    "                \n",
    "            \n",
    "            new_union = np.union1d(postings1, postings2)\n",
    "            \n",
    "            if union is None:\n",
    "                union = new_union \n",
    "            else: \n",
    "                union = np.union1d(union, new_union)\n",
    "        \n",
    "        intersection = np.intersect1d(intersection, inverted_index[query_words[i+1]])\n",
    "        union = np.union1d(union, inverted_index[query_words[i+1]])\n",
    "    \n",
    "    if len(intersection) >0:\n",
    "        return intersection\n",
    "    else:\n",
    "        return union\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    treebank = TreebankWordTokenizer()\n",
    "    #tokens = extractJson(hospitals, SEARCH_FIELDS)\n",
    "    remove = [\"_id\", \"createdAt\", \"updatedAt\", \"__v\", \"status\", \"isOccupied\", \"isVerified\"]\n",
    "    docs, words = getDocuments(hospitals, remove)\n",
    "    #print(docs)\n",
    "    docs = convert_to_lowercase(docs)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = list(set(cleanTokens(words, stop_words)))\n",
    "\n",
    "    var_idx, idx_var = buildIndices(hospitals, \"name\")\n",
    "    \n",
    "    index = inverted_index(docs)\n",
    "    #print(index)\n",
    "    \n",
    "    query = input(\"Enter hospital: \")\n",
    "    \n",
    "    print(retrieve_docs(query, index, treebank))\n",
    "\n",
    "    #build an inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter hospital: denver frimpong clinic\n",
      "[ 9 11]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
