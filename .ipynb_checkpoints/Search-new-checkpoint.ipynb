{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert json\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json data\n",
    "url = \"http://api.apomden.com/v2/facilities\"\n",
    "hospitals=requests.get(url).json()\n",
    "hospitals = hospitals[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_FIELDS = [\"name\", \"street\", \"city\", \"district\", \"regions\", \"services\"]\n",
    "\n",
    "def getTokens(sentence, tokenizer):\n",
    "    return tokenizer.tokenize(sentence)\n",
    "\n",
    "def extractJson(list_dicts, field_list):\n",
    "    \n",
    "    json_tokens = {field:[] for field in field_list}\n",
    "    \n",
    "    for item in list_dicts:\n",
    "        for field in SEARCH_FIELDS:\n",
    "            x = extract(item, field)\n",
    "            json_tokens[field].extend(x)\n",
    "    return json_tokens\n",
    "\n",
    "def extract(obj, key):\n",
    "    \n",
    "    tokens = []\n",
    "    if isinstance(obj,(str,bool)):\n",
    "        return [obj]\n",
    "    elif isinstance(obj,dict):\n",
    "        for k,v in obj.items():\n",
    "            if k==key:\n",
    "                tokens.append(v)\n",
    "            elif isinstance(v, dict):\n",
    "                if k in v.keys():\n",
    "                    tokens.extend(extract(v, key))\n",
    "            elif isinstance(v,(dict,list)):\n",
    "                tokens.extend(extract(v,key))\n",
    "                \n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            tokens.extend(extract(item,key))\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def vectorizeTokens(token_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a numpy matrix of vectorized tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def buildIndices(token_list, key=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns dictionaries of variable to idx and vice versa pairing.\n",
    "    token_list: list\n",
    "    key = string\n",
    "    \"\"\"\n",
    "              \n",
    "    var_to_index = {}\n",
    "    index_to_var = {}\n",
    "    \n",
    "    if key is not None:\n",
    "        for idx, item in enumerate(token_list):\n",
    "            var_to_index[item[key]] = idx\n",
    "            index_to_var[idx] =  item[key]      \n",
    "\n",
    "    else:\n",
    "        for idx, item in enumerate(token_list):\n",
    "            var_to_index[item] = idx\n",
    "            index_to_var[idx] = item\n",
    "\n",
    "    return var_to_index, index_to_var\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(d, exclude):\n",
    "    if not(isinstance(d, list)) and not(isinstance(d, dict)):\n",
    "        return [d]\n",
    "    tokens = []\n",
    "    if isinstance(d, list):\n",
    "        for i in d:\n",
    "            tokens.extend(get_all_tokens(i,exclude))\n",
    "    elif isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if k not in exclude:\n",
    "                tokens.extend(get_all_tokens(v,exclude))\n",
    "    return tokens\n",
    "\n",
    "def extract2(d, exclude):\n",
    "    tokens = []\n",
    "    if isinstance(d, list):\n",
    "        for i in d:\n",
    "            tokens.extend(get_all_tokens(i,exclude))\n",
    "    elif isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if k not in exclude:\n",
    "                tokens.extend(extract2(v, k))\n",
    "    return tokens\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    for k,v in d.items():\n",
    "        print(f\"------ {k} -----\")\n",
    "        if isinstance(v, list):\n",
    "            for i in v:\n",
    "                print(i)\n",
    "        else:\n",
    "            print(v)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocuments(list_dicts, exclude):\n",
    "    \n",
    "    def extractDocs(obj,key):\n",
    "        if key not in exclude:\n",
    "            if isinstance(obj,str):\n",
    "                return [obj]\n",
    "            elif isinstance(obj,(dict,list)) :\n",
    "                x = extract2(obj,exclude)\n",
    "                return x\n",
    "\n",
    "    docs = {}\n",
    "    all_words = []\n",
    "    for idx,doc in enumerate(list_dicts):\n",
    "        d = []\n",
    "        for k,v in doc.items():\n",
    "            if k not in exclude:\n",
    "                #print(k)\n",
    "                if not isinstance(v,(dict,list)):\n",
    "                    d.append(v)\n",
    "                    all_words.append(v)\n",
    "                else:\n",
    "                    val = extractDocs(v,k)\n",
    "                    d.extend(val)\n",
    "                    all_words.extend(val)\n",
    "        docs[idx] = d     \n",
    "\n",
    "    return docs, all_words\n",
    "\n",
    "\n",
    "def convert_to_lowercase(docs):  \n",
    "    new_docs = {}\n",
    "    for ix, lst in docs.items():\n",
    "        new_docs[ix] = list(map(lambda x:x.lower(),lst))\n",
    "    return new_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTokens(wordlist, stopwords):\n",
    "    x = []\n",
    "    for word in wordlist:\n",
    "        if isinstance(word,str) and word not in stopwords:\n",
    "            x.extend([wd.lower() for wd in word.strip().split()])\n",
    "    return x\n",
    "\n",
    "def inverted_index(docs):\n",
    "    \n",
    "    inverted_index=defaultdict(list)\n",
    "    \n",
    "    for ix, doc in docs.items():\n",
    "        for word in doc:\n",
    "            if ix not in inverted_index[word]:\n",
    "                inverted_index[word].append(ix)\n",
    "    \n",
    "    return inverted_index  \n",
    "\n",
    "def retrieve_docs(query, docs, tokenizer):\n",
    "    \n",
    "    query_words = getTokens(query, tokenizer)\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    treebank = TreebankWordTokenizer()\n",
    "    #tokens = extractJson(hospitals, SEARCH_FIELDS)\n",
    "    remove = [\"_id\", \"createdAt\", \"updatedAt\", \"__v\", \"status\", \"isOccupied\", \"isVerified\"]\n",
    "    docs, words = getDocuments(hospitals, remove)\n",
    "    docs = convert_to_lowercase(docs)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = list(set(cleanTokens(words, stop_words)))\n",
    "\n",
    "    var_idx, idx_var = buildIndices(hospitals, \"name\")\n",
    "    \n",
    "    index = inverted_index(docs)\n",
    "    print(index)\n",
    "\n",
    "    #build an inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'secondary': [0, 1, 4, 6, 9, 10, 11, 13, 14], '5d5b70925c0f901f2d91d190': [0], 'cocoaclinic': [0], 'cocoa clinic': [0], 'hospital': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'medical facility': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'male': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-1-b1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-1-b2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-1-b3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'general ward - room 1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'female': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-2-b1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-2-b2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'general ward - room 2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'unisex': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-3-b1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-3-b2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-3-b3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'gw-3-b4': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'general ward - room 3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'general ward': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-1-b1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-1-b2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-1-b3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu - room 1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-2-b1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-2-b2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu - room 2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-3-b1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-3-b2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-3-b3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu-4-b4': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu - room 3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'intensive care unit (icu)': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'general primary care': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], '': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'general consultation': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'icu': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], '5d5dd9265c0f901f2d91d275': [1], 'samhospital': [1], 'sam hospital': [1], 'tertiary': [2, 3, 12], '5d5dd95e5c0f901f2d91d2a3': [2], 'edgarhospital': [2], \"edgar's hospital\": [2], '5d5dd9b85c0f901f2d91d2d9': [3], 'osei': [3, 6], '5d5fbc3e5c0f901f2d91d451': [4], 'nanakclinic': [4], 'nana k clinic': [4], 'primary': [5, 7, 8], '5d71af105c0f901f2d91d533': [5], 'mabclinic': [5], 'mab clinic': [5], '5d729f4e5c0f901f2d91d5b7': [6], 'oseijk': [6], '5d7317c45c0f901f2d91d607': [7], 'ablekumaclinic': [7], 'ablekuma clinic': [7], '5d7466ac5c0f901f2d91d6a3': [8], 'seedclinic': [8], 'seed clinic': [8], '5d7d7f9a5c0f901f2d91d72c': [9], 'denver': [9], 'denver clinic': [9], '5d7fc901a7aba05d792e181f': [10], 'boateng': [10], '5d84656ca7aba05d792e1880': [11], 'frimpong': [11], 'frimpong clinic': [11], '5d8466b0510a2e644a11ac92': [12], 'lizzie': [12], '5d891a66a7aba05d792e1b70': [13], 'test': [13], 'test1': [13], '5d8ad6db3f763603fef9747c': [14], 'myhospital': [14]})\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
